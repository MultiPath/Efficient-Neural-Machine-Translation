In this dissertation, we have presented methods for developing an efficient neural machine translation system. 
We discussed  our methods tackling on twofolds: {\it data-efficient} and {\it decoding-efficient}.
In this section, we will conclude this thesis by chapters with detailed contributions, and discuss the challenges that are still faced by building a more efficient and also more effective neural machine translation, and suggest future work to tackle them.

In \textbf{Chapter~\ref{copy}}, we developed the copying mechanism as a new component  which targets on rote memories in general sequence-to-sequence (\sts) learning. The proposed \copynet also provided a general way in NMT to deal with out-of-vocabulary (OOV) words; 

In \textbf{Chapter~\ref{seg-nmt}}, we used a non-parametric search-engine to search similar translation pairs for guiding the target translation of a standard NMT system. The introduced method was a direct derivation from \copynet in NMT, but forms a novel non-parametric NMT system that was able to work on translation for professional domains; 

In \textbf{Chapter~\ref{ulr}}, we targeted on the other direction of data-inefficiency, which invents a universal NMT system for extremely low resource languages. Different from prior works on multi-lingual translation, we specially designed two additional modules which enable a jointly-trained model to work well on extremely low resource languages with around $6,000$ sentences; 

In \textbf{Chapter~\ref{MetaNMT}}, we extended the previously proposed universal NMT system to enable a pre-trained multilingual NMT model to adapt to any new languages efficiently. Moreover, the usage of meta-learning provided a principled way of finding a better initialization of parameters that is easy to fine-tune on new languages.


In \textbf{Chapter~\ref{trainable}}, we recasted the NMT decoding as a trainable process. The proposed {\it trainable greedy decoding} not only helped a trained model to decode more efficiently with greedy decoding while achieving better translation quality, but it opened an interesting direction where we can optimize the decoding towards any favorable objectives.

In \textbf{Chapter~\ref{nat}}, we invented the non-autoregressive NMT system that enabled translation in entirely parallel. The proposed method, with {\it fertility} as latent variables and sequence-level knowledge distillation, was one of the first NMT systems that was able to fundamentally remove the restriction of word-by-word decoding in conventional NMT models while keeping a close translation quality.

In \textbf{Chapter~\ref{simul}}, we developed the NMT model that learns to translate in real-time using reinforcement learning. More precisely, we formulated the simultaneous translation as a reinforcement learning setting where two actions -- read and write -- were taken on a pre-trained normal NMT.  The resulted model successfully achieved a balance between delay and quality. 


\section{Future Work}
We will conclude this dissertation with a discussion of the challenges to build an efficient neural machine translation that are still facing, which would be a clear goal of future work. 
\paragraph{Same quality, Better efficiency }
For most cases we have discussed in this dissertation, especially for the discussion of decoding-efficient NMT, we usually find algorithms with much better decoding efficiency with no matter non-autoregressive decoding (Chapter~\ref{nat}) or simultaneous translation (Chapter~\ref{simul}), which loses the translation quality like a paradox. 

Although real application sometimes cares more on efficiency than the degradation of the translation quality, it is still meaningful to stand on the current methods by exploring strategies which can keep the translation quality with almost no lost compared to the normal NMT system.  For example, it is reasonable trade-off to develop a semi-autoregressive NMT which learns to run parallel decoding only without hurting the translation accuracy; For simultaneous translation, we can also achieve a better  quality by learning to predict future words before outputting the target translation (Chapter~\ref{simul}).


\paragraph{Light-weight NMT}
In this thesis, we only discussed situations where the {\it efficient} NMT keeps the same or similar size of parameters as the baseline NMT model. Whereas, we did not cover another trend of building the efficient models by compressing the neural network parameters directly to a light-weight model so that it can fit for the computation resource of a moving device (e.g. mobile phone), which has also been investigated by years. A widely used method is to compress the model size with knowledge distillation to compress knowledge from a big pre-trained model~\cite{kim2016sequence}.  However, more investigation is still needed by combining the proposed methods in previous chapters, say, the light-weight simultaneous translation.

\paragraph{Incorporate Linguistic Information}
In  human translation, linguistic structures and similarity usually helps to derive the correct translation when the paired translation data is limited. Thus, it is also important to build a data-efficient by considering additional information from linguistic perspective. For example, it is possible to train multiple pre-trained models via meta-learning (Chapter~\ref{MetaNMT}) on a variety of high-resource languages, and match the associated linguistic features to guide any new coming language to find the best model for adaptation.

%\paragraph{Beyond Machine Translation}

