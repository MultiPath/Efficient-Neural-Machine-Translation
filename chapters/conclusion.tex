In this dissertation, we have presented methods for developing an efficient neural machine translation system. 
We discussed  our methods to tackle on {\it data-efficiency} and {\it decoding-efficiency}.
In this section, we will conclude this thesis by summarizing the contributions of each chapter, and discussing the challenges still remaining in building a more efficient and also more effective neural machine translation, and suggesting future work to tackle them.

In \textbf{Chapter~\ref{copy}}, we developed the copying mechanism as a new component  which targets rote memories in general sequence-to-sequence (\sts) learning. The proposed \copynet also provides a general way in NMT to deal with out-of-vocabulary (OOV) words; 

In \textbf{Chapter~\ref{seg-nmt}}, we used a non-parametric search-engine to search similar translation pairs for guiding the target translation of a standard NMT system. The introduced method was a direct derivation from \copynet in NMT, but forms a novel non-parametric NMT system  able to work on translation for professional domains; 

In \textbf{Chapter~\ref{ulr}}, we targeted on the other direction of data-inefficiency, and invents a universal NMT system for extremely low resource languages. Different from prior works on multi-lingual translation, we specially designed two additional modules which enable a jointly-trained model to work well on extremely low resource languages with around $6,000$ sentences; 

In \textbf{Chapter~\ref{MetaNMT}}, we extended the previously proposed universal NMT system to enable a pre-trained multilingual NMT model to adapt to any new languages efficiently. Moreover, the usage of meta-learning provides a  novel  view of finding a better initialization of parameters that is easy to fine-tune on new languages.

In \textbf{Chapter~\ref{trainable}}, we recasted the NMT decoding as a trainable process. The proposed {\it trainable greedy decoding} not only helped a trained model to decode more efficiently with greedy decoding while achieving better translation quality, but also introduced an interesting direction where we can optimize the decoding towards any favorable objectives.

In \textbf{Chapter~\ref{nat}}, we invented the non-autoregressive NMT system that enabled translation in parallel. The proposed method, with {\it fertility} as latent variables and sequence-level knowledge distillation, was one of the first NMT systems that was able to fundamentally remove the restriction of word-by-word decoding in conventional NMT models while keeping a similar translation quality.

In \textbf{Chapter~\ref{simul}}, we developed the NMT model that learns to translate in real-time using reinforcement learning. More precisely, we formulated the simultaneous translation as a reinforcement learning setting where two actions -- read and write -- were taken on a pre-trained normal NMT.  The resulted model successfully achieved a balance between delay and quality. 


\section{Future Work}
We will conclude this dissertation with a discussion of the challenges that remain in building efficient neural machine translation system.
\paragraph{Same quality, better efficiency }
In most cases we have discussed in this dissertation, especially for the discussion of decoding-efficient NMT, we usually find algorithms with much better decoding efficiency ( for example, the non-autoregressive translation (Chapter~\ref{nat}) or simultaneous translation (Chapter~\ref{simul})), whereas that loses a lot considering the translation quality. 

Although real application sometimes cares more about efficiency than the degradation of the translation quality, it is still meaningful to build on the current methods by exploring strategies which can keep the translation quality with almost no loss compared to the normal NMT system.  For example, it is reasonable to develop a semi-autoregressive NMT which learns to run parallel decoding only without hurting the translation accuracy. For simultaneous translation, we can also achieve a better  quality by learning to predict future words before outputting the target translation (Chapter~\ref{simul}).


\paragraph{Light-weight NMT}
In this thesis, we only discussed situations where the {\it efficient} NMT keeps the same or similar size of parameters as the baseline NMT model. Whereas, we did not cover another trend of building the efficient models by compressing the neural network parameters directly to a light-weight model so that it can fit the computation resource of a portable device (e.g. mobile phone), which has also been investigated for years. A widely used method is to compress the model size with knowledge distillation to compress knowledge from a big pre-trained model~\cite{kim2016sequence}.  However, more investigation is still needed by combining the proposed methods in previous chapters, say, the light-weight simultaneous translation.

\paragraph{Incorporating Linguistic Information}
In  human translation, linguistic structures and similarity usually help to derive the correct translation when the paired translation data is limited. Thus, it is also important to build a data-efficient system by considering additional information from  a linguistic perspective. For example, it is possible to train multiple pre-trained models via meta-learning (Chapter~\ref{MetaNMT}) on a variety of high-resource languages, and match the associated linguistic features to guide any new language to find the best model for adaptation.

%\paragraph{Beyond Machine Translation}

