\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Examples of synthetic rules and examples\relax }}{31}{table.caption.30}
\contentsline {table}{\numberline {3.2}{\ignorespaces The test accuracy (\%) on synthetic data.\relax }}{32}{table.caption.32}
\contentsline {table}{\numberline {3.3}{\ignorespaces The statistics of the LCSTS dataset.\relax }}{33}{table.caption.35}
\contentsline {table}{\numberline {3.4}{\ignorespaces Testing performance of LCSTS, where ``RNN" is canonical Enc-Dec, and ``RNN context" its attentive variant.\relax }}{34}{table.caption.37}
\contentsline {table}{\numberline {3.5}{\ignorespaces The decoding accuracy on the two testing sets. Decoding is admitted success only when the answer is found exactly in the Top-K outputs. \relax }}{38}{table.caption.43}
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Statistics from the JRC-Acquis corpus. We use BPE subword symbols.\relax }}{51}{table.caption.55}
\contentsline {table}{\numberline {4.2}{\ignorespaces The BLEU scores on JRC-Acquis corpus.\relax }}{52}{table.caption.58}
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces Statistics of the available parallel resource for extremely low-resource languages in our experiments. All are translated to English.\relax }}{69}{table.caption.78}
\contentsline {table}{\numberline {5.2}{\ignorespaces Statistics of the available parallel resource in our experiments. All the languages are translated to English.\relax }}{69}{table.caption.79}
\contentsline {table}{\numberline {5.3}{\ignorespaces Scores over variant source languages (6k sentences for Ro \& Lv, and 10k for Ko). ``Multi" means the Multi-lingual NMT baseline.\relax }}{71}{table.caption.85}
\contentsline {table}{\numberline {5.4}{\ignorespaces BLEU scores evaluated on test set (6k), compared with ULR and MoLE. ``vanilla" is the standard NMT system trained only on Ro-En training set\relax }}{73}{table.caption.89}
\addvspace {10\p@ }
\contentsline {table}{\numberline {6.1}{\ignorespaces Statistics of full datasets of the target language pairs. BLEU scores on the dev and test sets are reported from a supervised Transformer model with the same architecture.\relax }}{87}{table.caption.106}
\contentsline {table}{\numberline {6.2}{\ignorespaces BLEU Scores w.r.t. the source task set for all five target tasks.\relax }}{92}{table.caption.119}
\contentsline {table}{\numberline {6.3}{\ignorespaces Sample translations for Tr-En and Ko-En highlight the impact of fine-tuning which results in syntactically better formed translations. We highlight tokens of interest in terms of reordering. \relax }}{93}{table.caption.120}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {8.1}{\ignorespaces Dataset statistics (\# of sentence pairs).\relax }}{129}{table.caption.156}
\contentsline {table}{\numberline {8.2}{\ignorespaces BLEU scores on official test sets (\texttt {newstest2014} for WMT En-De and \texttt {newstest2016} for WMT En-Ro) or the development set for IWSLT. NAT models without NPD use argmax decoding. Latency is computed as the time to decode a single sentence without minibatching, averaged over the whole test set; decoding is implemented in PyTorch on a single NVIDIA Tesla P100.\relax }}{131}{table.caption.165}
\contentsline {table}{\numberline {8.3}{\ignorespaces Ablation performance on the IWSLT development set. BLEU (T) refers to the BLEU score on a version of the development set that has been translated by the teacher model. An $\times $ indicates that fine-tuning caused that model to get worse. When uniform copying is used as the decoder inputs, the ground-truth target lengths are provided. All models use argmax decoding.\relax }}{132}{table.caption.167}
\addvspace {10\p@ }
\addvspace {10\p@ }
