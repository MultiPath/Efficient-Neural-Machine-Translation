\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces The aim in building Babel and a Tower to “reach into heaven” was to prevent the people from being “scattered abroad over the face of the whole earth” (Genesis 11:4). \relax }}{2}{figure.caption.6}
\contentsline {figure}{\numberline {1.2}{\ignorespaces A comparison between NMT and SMT given varying sizes of training examples. Curves extracted from \citet {koehn2017six}.\relax }}{4}{figure.caption.11}
\contentsline {figure}{\numberline {1.3}{\ignorespaces The goal of the efficient NMT decoding -- simultaneous translation.\relax }}{5}{figure.caption.12}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces An illustration of the comparison between the conventional {{\textsc {Seq2Seq}}}\xspace learning and {{\textsc {Seq2Seq}}}\xspace with attention mechanism for translating ``A B C D $\rightarrow $ X Y Z ''.\relax }}{11}{figure.caption.16}
\contentsline {figure}{\numberline {2.2}{\ignorespaces A flow-chart of the Transformer model from \citet {vaswani2017attention}. \relax }}{13}{figure.caption.17}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The overall diagram of {\textsc {CopyNet}}\xspace . For simplicity, we omit some links for prediction. \relax }}{20}{figure.caption.20}
\contentsline {figure}{\numberline {3.2}{\ignorespaces The illustration of the decoding probability $p(y_t|\cdot )$ as a 4-class classifier. \relax }}{23}{figure.caption.25}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Example output of {\textsc {CopyNet}}\xspace on the synthetic dataset. The heatmap represents the activations of the copy-mode over the input sequence (left) during the decoding process (bottom).\relax }}{28}{figure.caption.31}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Examples of {\textsc {CopyNet}}\xspace on LCSTS compared with RNN context. Word segmentation is applied on the input, where underlined are OOV words. The highlighted words (with different colors) are those words with copy-mode probability higher than the generate-mode. We also provide literal English translation for the document, the golden, and {\textsc {CopyNet}}\xspace , while omitting that for RNN context since the language is broken.\relax }}{30}{figure.caption.36}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Examples on the testing set of DS-II shown as the input text and golden, with the outputs of RNNSearch and CopyNet. Words in red rectangles are unseen in the training set. The highlighted words (with different colors) are those words with copy-mode probability higher than the generate-mode. Green cirles (meaning correct) and red cross (meaning incorrect) are given based on human judgment on whether the response is appropriate. \relax }}{32}{figure.caption.40}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces An illustration of the proposed search-engine guided non-parametric neural machine translation.\relax }}{36}{figure.caption.43}
\contentsline {figure}{\numberline {4.2}{\ignorespaces The overall architecture of the proposed SEG-NMT. The shaded box includes the module which handles a set of translation pairs retrieved in the first stage. The heat maps represent the attention scores between the source sentences (left-to-right) and the corresponding translations (top-to-bottom).\relax }}{38}{figure.caption.44}
\contentsline {figure}{\numberline {4.3}{\ignorespaces The improvement over the baseline by SEG-NMT on Fr$\to $En w.r.t. the fuzzy matching scores of one retrieved translation pair. \relax }}{47}{figure.caption.58}
\contentsline {figure}{\numberline {4.4}{\ignorespaces The BLEU scores on Fr$\to $En using varying numbers of retrieved translation pairs during testing. The model was trained once. ``Adaptive'' refers to the proposed greedy selection in Alg.\nobreakspace {}\ref {cp4.algo.alg1}. \relax }}{47}{figure.caption.59}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Three examples from the Fr$\to $En test set. For the proposed SEG-NMT model, one translation pair is retrieved from the training set. Each token in the translation by the proposed approach and its corresponded token (if it exists) in the retrieved pair are shaded in blue according to the gating variable $\zeta _t$ from Eq.\nobreakspace {}\textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {cp4.eq.shallow}\unskip \@@italiccorr )}}. In all, we show: (S) the source sentence. (RS) the source side of a retrieved pair. (RT) the target side of the retrieved pair. (A) the translation by the proposed approach. (B) the translation by the baseline. (T) the reference translation. \relax }}{49}{figure.caption.63}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces BLEU scores reported on the test set for Ro-En. The amount of training data effects the translation performance dramatically using a single NMT model.\relax }}{54}{figure.caption.65}
\contentsline {figure}{\numberline {5.2}{\ignorespaces word ``cat'' in different languages\relax }}{56}{figure.caption.67}
\contentsline {figure}{\numberline {5.3}{\ignorespaces An illustration of the proposed architecture of ULR and MoLE. Shaded parts are trained within the NMT model while unshaded parts are not changed during training.\relax }}{57}{figure.caption.69}
\contentsline {figure}{\numberline {5.4}{\ignorespaces An illustration of projecting multiple monolingual embeddings (Es, Fr, It, Pt, Ro) to the same universal (En) space.\relax }}{60}{figure.caption.72}
\contentsline {figure}{\numberline {5.5}{\ignorespaces BLEU score vs corpus size\relax }}{66}{figure.caption.84}
\contentsline {figure}{\numberline {5.6}{\ignorespaces BLEU score vs unknown tokens\relax }}{66}{figure.caption.85}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Three sets of examples on Ro-En translation with variant settings. \relax }}{69}{figure.caption.89}
\contentsline {figure}{\numberline {5.8}{\ignorespaces The activation visualization of mixture of language experts module on one randomly selected Ro source sentences trained together with different auxiliary languages. Darker color means higher activation score. \relax }}{70}{figure.caption.90}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces The graphical illustration of the training process of the proposed MetaNMT. For each episode, one task (language pair) is sampled for meta-learning. The boxes and arrows in blue are mainly involved in language-specific learning (\textsection \ref {cp6.sec.lsl}), and those in purple in meta-learning (\textsection \ref {cp6.sec.ml}).\relax }}{76}{figure.caption.95}
\contentsline {figure}{\numberline {6.2}{\ignorespaces An intuitive illustration in which we use solid lines to represent the learning of initialization, and dashed lines to show the path of fine-tuning.\relax }}{79}{figure.6.2}
\contentsline {figure}{\numberline {6.3}{\ignorespaces BLEU scores reported on test sets for \{Ro, Lv, Fi, Tr\} to En, where each model is first learned from 6 source tasks (Es, Fr, It, Pt, De, Ru) and then fine-tuned on randomly sampled training sets with around 16,000 English tokens per run. The error bars show the standard deviation calculated from 5 runs.\relax }}{84}{figure.caption.111}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Ro-En}}}{84}{figure.caption.111}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Lv-En}}}{84}{figure.caption.111}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Fi-En}}}{84}{figure.caption.111}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Tr-En}}}{84}{figure.caption.111}
\contentsline {figure}{\numberline {6.4}{\ignorespaces BLEU Scores w.r.t. the size of the target task's training set.\relax }}{85}{figure.caption.114}
\contentsline {figure}{\numberline {6.5}{\ignorespaces The learning curves of BLEU scores on the validation task (Ro-En).\relax }}{87}{figure.caption.116}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces The graphical illustration of the proposal trainable greedy decoding. $\theta $, $\phi $ and $\psi $ respectively correspond to the parameters of the underlying neural machine translation model, the trainable greedy decoding (actor) and the critic. \relax }}{94}{figure.caption.122}
\contentsline {figure}{\numberline {7.2}{\ignorespaces Graphical illustrations of the trainable greedy decoding. The left panel shows a single step of the actor interacting with the underlying neural translation model, and The right panel the interaction among the underlying neural translation system (dashed-border boxes), actor (red-border boxes), and critic (blue-border boxes). The solid arrows indicate the forward pass, and the dashed yellow arrows the actor's backward pass. The dotted-border box shows the use of a reference translation.\relax }}{99}{figure.caption.127}
\contentsline {figure}{\numberline {7.3}{\ignorespaces The plots draw the improvements by the trainable greedy decoding on the test set.}}{104}{figure.caption.134}
\contentsline {figure}{\numberline {7.4}{\ignorespaces Comparison of greedy BLEU scores whether using the critic-aware exploration or not on Ru-En Dataset. The green line means the BLEU score achieved by greedy decoding from the underlying NMT model.\relax }}{105}{figure.caption.135}
\contentsline {figure}{\numberline {7.5}{\ignorespaces Three Ru-En examples in which the difference between the trainable greedy decoding (A) and the conventional greedy decoding (G) is large. Each step is marked with magenta, when the actor significantly influenced the output distribution.\relax }}{105}{figure.caption.136}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {8.1}{\ignorespaces Translating ``A B C'' to ``X Y'' using autoregressive and non-autoregressive neural MT architectures. The latter generates all output tokens in parallel.\relax }}{108}{figure.caption.139}
\contentsline {figure}{\numberline {8.2}{\ignorespaces The architecture of the NAT{}, where the black solid arrows represent differentiable connections and the purple dashed arrows are non-differentiable operations. Each sublayer inside the encoder and decoder stacks also includes layer normalization and a residual connection.\relax }}{110}{figure.caption.142}
\contentsline {figure}{\numberline {8.3}{\ignorespaces BLEU scores on IWSLT development set as a function of sample size for noisy parallel decoding. NPD matches the performance of the other two decoding strategies after two samples, and exceeds the performance of the autoregressive teacher with around 1000.\relax }}{120}{figure.caption.156}
\contentsline {figure}{\numberline {8.4}{\ignorespaces Two examples comparing translations produced by an autoregressive (AR) and non-autoregressive Transformer as well as the result of noisy parallel decoding with sample size 100. Repeated words are highlighted in gray.\relax }}{123}{figure.caption.166}
\contentsline {figure}{\numberline {8.5}{\ignorespaces A Romanian--English example translated with noisy parallel decoding. At left are eight sampled fertility sequences from the encoder, represented with their corresponding decoder input sequences. Each of these values for the latent variable leads to a different possible output translation, shown at right. The autoregressive Transformer then picks the best translation, shown in red, a process which is much faster than directly using it to generate output.\relax }}{123}{figure.caption.167}
\contentsline {figure}{\numberline {8.6}{\ignorespaces The translation latency, computed as the time to decode a single sentence without minibatching, for each sentence in the IWSLT development set as a function of its length. The autoregressive model has latency linear in the decoding length, while the latency of the NAT is nearly constant for typical lengths, even with NPD with sample size 10. When using NPD with sample size 100, the level of parallelism is enough to more than saturate the GPU, leading again to linear latencies.\relax }}{124}{figure.caption.168}
\contentsline {figure}{\numberline {8.7}{\ignorespaces Learning curves for training and fine-tuning of the NAT{} on IWSLT. BLEU scores are on the development set.\relax }}{124}{figure.caption.169}
\contentsline {figure}{\numberline {8.8}{\ignorespaces The schematic structure of training and inference for the NAT. The ``distilled data'' contains target sentences decoded by the autoregressive model and ground-truth source sentences.\relax }}{125}{figure.caption.171}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {9.1}{\ignorespaces {Example output from the proposed framework in DE $\rightarrow $ EN simultaneous translation. The heat-map represents the soft alignment between the incoming source sentence (left, up-to-down) and the emitted translation (top, left-to-right). The length of each column represents the number of source words being waited for before emitting the translation. Best viewed when zoomed digitally.}\relax }}{128}{figure.caption.172}
\contentsline {figure}{\numberline {9.2}{\ignorespaces {Illustration of the proposed framework: at each step, the NMT environment (left) computes a candidate translation. The recurrent agent (right) will the observation including the candidates and send back decisions--\textsc {read} or \textsc {write}.}\relax }}{130}{figure.caption.173}
\contentsline {figure}{\numberline {9.3}{\ignorespaces {Illustrations of (A)\nobreakspace {}beam-search, (B)\nobreakspace {}simultaneous greedy decoding and (C)\nobreakspace {}simultaneous beam-search.}\relax }}{137}{figure.caption.184}
\contentsline {figure}{\numberline {9.4}{\ignorespaces {Learning progress curves for variant delay targets on the validation dataset for EN $\rightarrow $ RU. Every time we only keep one target for one delay measure. For instance when using target AP, the coefficient of $\alpha $ in Eq.\nobreakspace {}\textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {cp9.eq.rd}\unskip \@@italiccorr )}} will be set $0$.}\relax }}{140}{figure.caption.188}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {BLEU (EN $\rightarrow $ RU)}}}{140}{figure.caption.188}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {AP (EN $\rightarrow $ RU)}}}{140}{figure.caption.188}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {CW (EN $\rightarrow $ RU)}}}{140}{figure.caption.188}
\contentsline {figure}{\numberline {9.5}{\ignorespaces {Delay\nobreakspace {}(AP) v.s. BLEU for both language pair--directions. The shown point-pairs are the results of simultaneous greedy decoding and beam-search (beam-size = 5) respectively with models trained for various delay targets: ($\color {green!70!blue}\blacktriangleleft \triangleleft $: CW=$8$, $\color {green!70!blue}\blacktriangle \triangle $: CW=$5$, $\color {green!70!blue}\blacklozenge \lozenge $: CW=$2$, $\color {red} \blacktriangleright \triangleright $: AP=$0.3$, $\color {red} \blacktriangledown \triangledown $: AP=$0.5$, $\color {red} \blacksquare \square $: AP=$0.7$)}. For each target, we select the model that maximizes the quality-to-delay ratio ($\frac {\text {BLEU}}{\text {AP}}$) on the validation set. The baselines are also plotted ($\color {blue}\bigstar $: WOS $\color {black}\bigstar ${\fontfamily {pzd}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 73}: WUE, $\times $: WID, $+$: WIW).\relax }}{141}{figure.caption.189}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {EN$\rightarrow $RU}}}{141}{figure.caption.189}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {RU$\rightarrow $EN}}}{141}{figure.caption.189}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {EN$\rightarrow $DE}}}{141}{figure.caption.189}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {DE$\rightarrow $EN}}}{141}{figure.caption.189}
\contentsline {figure}{\numberline {9.6}{\ignorespaces {Delay\nobreakspace {}(CW) v.s. BLEU score for EN $\rightarrow $ RU, ($\color {green!70!blue}\blacktriangleleft \triangleleft $: CW=$8$, $\color {green!70!blue}\blacktriangle \triangle $: CW=$5$, $\color {green!70!blue}\blacklozenge \lozenge $: CW=$2$, $\color {red} \blacktriangleright \triangleright $: AP=$0.3$, $\color {red} \blacktriangledown \triangledown $: AP=$0.5$, $\color {red} \blacksquare \square $: AP=$0.7$), against the baselines\nobreakspace {}($\color {blue}\bigstar $: WOS $\color {black}\bigstar $: WUE, $+$: SEG1, $\times $: SEG2).}\relax }}{142}{figure.caption.190}
\contentsline {figure}{\numberline {9.7}{\ignorespaces {Given the example input sentence (leftmost column), we show outputs by models trained for various delay targets. For these outputs, each row corresponds to one source word and represents the emitted words (maybe empty) after reading this word. The corresponding source and target words are in the same color for all model outputs.}\relax }}{144}{figure.caption.195}
\contentsline {figure}{\numberline {9.8}{\ignorespaces {Comparison of DE$\rightarrow $EN examples using the proposed framework and usual NMT system respectively. Both the heatmaps share the same setting with Fig.\nobreakspace {}\ref {cp9.fig.crop}}. The verb ``gedeckt'' is incorrectly translated in simultaneous translation.\relax }}{145}{figure.caption.196}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Simultaneous Neural Machine Translation}}}{145}{figure.caption.196}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Neural Machine Translation}}}{145}{figure.caption.196}
\addvspace {10\p@ }
